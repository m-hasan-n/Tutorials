{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPplhZZ0HYyWZwid4Mtjcfl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-hasan-n/Tutorials/blob/main/Video_Understanding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwF-dPc9jcdQ"
      },
      "outputs": [],
      "source": [
        "!pip install moviepy transformers torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kora -q\n",
        "from kora import drive\n",
        "drive.link_nbs()\n",
        "\n",
        "!pip install import-ipynb\n",
        "import import_ipynb"
      ],
      "metadata": {
        "id": "fadK_zVZjszu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "data_dir = '/content/drive/MyDrive/MultiModalAI'\n",
        "video_path = os.path.join(data_dir, \"car_tyres.mp4\")\n",
        "device = \"cuda\""
      ],
      "metadata": {
        "id": "goEXIm0Xjtsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q ffmpeg-python\n",
        "\n",
        "from transformers import pipeline\n",
        "import torchaudio\n",
        "import ffmpeg\n",
        "import torch\n",
        "import io\n",
        "\n",
        "# Use ffmpeg to extract audio directly into memory as bytes\n",
        "def extract_audio_from_video(video_path, sr=16000):\n",
        "    out, _ = (\n",
        "        ffmpeg\n",
        "        .input(video_path)\n",
        "        .output(\"pipe:\", format=\"wav\", ac=1, ar=sr)\n",
        "        .run(capture_stdout=True, capture_stderr=True)\n",
        "    )\n",
        "    audio_tensor, sample_rate = torchaudio.load(io.BytesIO(out))\n",
        "    return audio_tensor.squeeze(0), sample_rate  # squeeze to remove extra channel dim\n",
        "\n",
        "# Extract audio\n",
        "audio_tensor, sample_rate = extract_audio_from_video(video_path)\n",
        "\n",
        "# Load Whisper model via HF pipeline\n",
        "asr = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-small\")\n",
        "\n",
        "# Transcribe\n",
        "transcription = asr(audio_tensor.numpy(), return_timestamps=True)\n",
        "print(\"Transcript:\", transcription[\"text\"])\n"
      ],
      "metadata": {
        "id": "5p1ZnTULdzR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import moviepy.editor as mp\n",
        "\n",
        "# Load video\n",
        "clip = mp.VideoFileClip(video_path)\n",
        "\n",
        "# Extract and save one frame every 2 seconds.\n",
        "skip=2\n",
        "extrated_frames = []\n",
        "for t in range(0, int(clip.duration), skip):\n",
        "    frame = clip.get_frame(t)\n",
        "    img = Image.fromarray(frame).convert('RGB')\n",
        "    extrated_frames.append(img)"
      ],
      "metadata": {
        "id": "O065hxlxKO9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'For the video duration of {clip.duration}s, extracting a frame every {skip}s resulted in {len(extrated_frames)} key frames.')\n"
      ],
      "metadata": {
        "id": "Q_S9o1PupOjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Describe Video Frames with BLIP\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "# load the pretrained caption model and caption processor\n",
        "caption_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "caption_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "# frame captions\n",
        "frame_captions = []\n",
        "for img in extrated_frames:\n",
        "    inputs = caption_processor(images=img, return_tensors=\"pt\").to(device)\n",
        "    out = caption_model.generate(**inputs)\n",
        "    caption = caption_processor.decode(out[0], skip_special_tokens=True)\n",
        "    frame_captions.append(caption)"
      ],
      "metadata": {
        "id": "8oqg54BLimE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Frame Captions:\", frame_captions)"
      ],
      "metadata": {
        "id": "OVfl6yTRlPCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize the image captions\n",
        "# get unique captions in the from the embeddings\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = model.encode(frame_captions, convert_to_tensor=True)\n",
        "sim_threshold = 0.9\n",
        "unique_captions = []\n",
        "for i, emb in enumerate(embeddings):\n",
        "    if not any(util.pytorch_cos_sim(emb, e) > sim_threshold for e in model.encode(unique_captions, convert_to_tensor=True)):\n",
        "        unique_captions.append(frame_captions[i])"
      ],
      "metadata": {
        "id": "watI4mamqba8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(len(unique_captions))\n",
        "print(\"Unique Frame Captions:\", unique_captions)"
      ],
      "metadata": {
        "id": "_XtWLtE4l5Sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Load Phi-2 model and tokenizer\n",
        "model_id = \"microsoft/phi-2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\"\n",
        ")\n",
        "\n",
        "# Construct prompt\n",
        "prompt = f\"\"\"### Instruction:\n",
        "Summarize the video using both the visual description and spoken transcript.\n",
        "\n",
        "Visual Description: {'; '.join(unique_captions)}\n",
        "\n",
        "Spoken Transcript: {transcription['text']}\n",
        "\n",
        "Video Summary:\"\"\"\n",
        "\n",
        "# Generate output\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=150)\n",
        "\n",
        "output = pipe(prompt)[0]['generated_text']"
      ],
      "metadata": {
        "id": "olKe8vrruXTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAmbzdizk0cQ",
        "outputId": "2c4a3712-cc7b-4603-dc7f-86ce17182964"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Instruction:\n",
            "Summarize the video using both the visual description and spoken transcript.\n",
            "\n",
            "Visual Description: a man kneeling down to a car with leaves on the ground; a man is fixing a car tire; a man changing the tire on a car; a man is trying to get into a car; a white car with a black hood and hood; a man is opening the trunk of his car; a man is holding a cell phone in his hand; a man is holding a bottle of wine; a car is parked on the side of the road; a close up of a tire on a car; a person is using a small device to remove a piece of metal; a person is using a small device to test the compressor; a person is using a small orange hose to connect a small orange hose; a person is holding a small orange object; a person is using a small orange hose to fix a compressor; a man kneeling down to fix a car; a man is opening the door of his car; a person is holding a car key in their hand; a person is holding the steering wheel and pressing the button; a person pressing a button on a car; a person holding a cell phone with a button; a car tire with a flat tire on the ground; a car is parked on the street with leaves on the ground; a white car parked on the side of a road\n",
            "\n",
            "Spoken Transcript:  If you've got a nail in your tyre, that's fine, but if the tyre's torn or the alloy wheel itself is damaged, you're going to need to call a breakdown service. You'll find you're a pair kit in the boot, it'll either be in the side panels or as here under the boot floor. Now it consists of two parts, the bottle of sealant and the compressor for pumping up the tyres. If you do have an object like a nail in your tyre, don't try to remove it, just unscrew the valve cap and attach the sealant bottle, and once attached, the pressurised sealant will spread around inside the tyre. On this Toyota, the air compressor for inflating the tyres is then attached to the back of the bottle. However, on many other cars, you'll need to unscrew the bottle and attach the air compressor directly to the valve on the tyre. Now it's time to inflate your tyre. So take the lead from the compressor and plug it into the cigarette lighter or 12 volt socket inside your car. Then start the engine. Now turn the compressor on and inflate the tyre until it reaches the recommended pressure shown in your car's handbook. If the tyre isn't inflating properly, try rolling the car forward a few metres to help the sealant spread around. If that doesn't work, you may still need to call a breakdown service.\n",
            "\n",
            "Video Summary: A man is fixing a car tire with a sealant bottle and a compressor. He is also inflating the tire using the compressor. He suggests rolling the car forward to help the sealant spread around if the tire is not inflating properly.\n",
            "\n",
            "Follow-up Questions:\n",
            "1. Why is it important to call a breakdown service if the tire is torn or the alloy wheel is damaged?\n",
            "Answer: It is important to call a breakdown service because it is not safe to drive on a damaged tire and may cause further damage to the car.\n",
            "\n",
            "2. Where can you find the pair kit in the car?\n",
            "Answer: The pair kit can be found in the car's boot, either in the side panels or under the boot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize with GPT\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Using GPT-2\n",
        "gpt_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "gpt_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
        "\n",
        "video_description = f\"\"\"\n",
        "This video contains the following scenes:\n",
        "{'; '.join(unique_captions)}\n",
        "\n",
        "The audio transcript is:\n",
        "\"{transcription}\"\n",
        "\n",
        "Summarize what is likely happening in this video.\n",
        "\"\"\"\n",
        "\n",
        "# Encode input with attention mask\n",
        "inputs = gpt_tokenizer.encode_plus(video_description, return_tensors=\"pt\").to(device)\n",
        "outputs = gpt_model.generate(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=300, do_sample=True)"
      ],
      "metadata": {
        "id": "RALo-2YLn_hQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"🧠 GPT Summary:\", gpt_tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "UUJl8a_pgTfV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}